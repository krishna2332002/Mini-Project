{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOM/zNG/01Pb7YUzlF0Xl/5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishna2332002/Mini-Project/blob/main/CW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w5AhbsXaDqW",
        "outputId": "8736807b-1cc7-4d3e-9082-77a6f612d31b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from timeit import default_timer\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')           # noqa: E402\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cw(model, x, y=None, eps=1.0, ord_=2, T=2,\n",
        "       optimizer=tf.train.AdamOptimizer(learning_rate=0.1), alpha=0.9,\n",
        "       min_prob=0, clip=(0.0, 1.0)):\n",
        "    \"\"\"CarliniWagner (CW) attack.\n",
        "    Only CW-L2 and CW-Linf are implemented since I do not see the point of\n",
        "    embedding CW-L2 in CW-L1.  See https://arxiv.org/abs/1608.04644 for\n",
        "    details.\n",
        "    The idea of CW attack is to minimize a loss that comprises two parts: a)\n",
        "    the p-norm distance between the original image and the adversarial image,\n",
        "    and b) a term that encourages the incorrect classification of the\n",
        "    adversarial images.\n",
        "    Please note that CW is a optimization process, so it is tricky.  There are\n",
        "    lots of hyper-parameters to tune in order to get the best result.  The\n",
        "    binary search process for the best eps values is omitted here.  You could\n",
        "    do grid search to find the best parameter configuration, if you like.  I\n",
        "    demonstrate binary search for the best result in an example code.\n",
        "    :param model: The model wrapper.\n",
        "    :param x: The input clean sample, usually a placeholder.  NOTE that the\n",
        "              shape of x MUST be static, i.e., fixed when constructing the\n",
        "              graph.  This is because there are some variables that depends\n",
        "              upon this shape.\n",
        "    :param y: The target label.  Set to be the least-likely label when None.\n",
        "    :param eps: The scaling factor for the second penalty term.\n",
        "    :param ord_: The p-norm, 2 or inf.  Actually I only test whether it is 2\n",
        "        or not 2.\n",
        "    :param T: The temperature for sigmoid function.  In the original paper,\n",
        "              the author used (tanh(x)+1)/2 = sigmoid(2x), i.e., t=2.  During\n",
        "              our experiment, we found that this parameter also affects the\n",
        "              quality of generated adversarial samples.\n",
        "    :param optimizer: The optimizer used to minimize the CW loss.  Default to\n",
        "        be tf.AdamOptimizer with learning rate 0.1. Note the learning rate is\n",
        "        much larger than normal learning rate.\n",
        "    :param alpha: Used only in CW-L0.  The decreasing factor for the upper\n",
        "        bound of noise.\n",
        "    :param min_prob: The minimum confidence of adversarial examples.\n",
        "        Generally larger min_prob wil lresult in more noise.\n",
        "    :param clip: A tuple (clip_min, clip_max), which denotes the range of\n",
        "        values in x.\n",
        "    :return: A tuple (train_op, xadv, noise).  Run train_op for some epochs to\n",
        "             generate the adversarial image, then run xadv to get the final\n",
        "             adversarial image.  Noise is in the sigmoid-space instead of the\n",
        "             input space.  It is returned because we need to clear noise\n",
        "             before each batched attacks.\n",
        "    \"\"\"\n",
        "    xshape = x.get_shape().as_list()\n",
        "    noise = tf.get_variable('noise', xshape, tf.float32,\n",
        "                            initializer=tf.initializers.zeros)\n",
        "\n",
        "    # scale input to (0, 1)\n",
        "    x_scaled = (x - clip[0]) / (clip[1] - clip[0])\n",
        "\n",
        "    # change to sigmoid-space, clip to avoid overflow.\n",
        "    z = tf.clip_by_value(x_scaled, 1e-8, 1-1e-8)\n",
        "    xinv = tf.log(z / (1 - z)) / T\n",
        "\n",
        "    # add noise in sigmoid-space and map back to input domain\n",
        "    xadv = tf.sigmoid(T * (xinv + noise))\n",
        "    xadv = xadv * (clip[1] - clip[0]) + clip[0]\n",
        "\n",
        "    ybar, logits = model(xadv, logits=True)\n",
        "    ydim = ybar.get_shape().as_list()[1]\n",
        "    if y is not None:\n",
        "        y = tf.cond(tf.equal(tf.rank(y), 0),\n",
        "                    lambda: tf.fill([xshape[0]], y),\n",
        "                    lambda: tf.identity(y))\n",
        "    else:\n",
        "        # we set target to the least-likely label\n",
        "        y = tf.argmin(ybar, axis=1, output_type=tf.int32)\n",
        "\n",
        "    mask = tf.one_hot(y, ydim, on_value=0.0, off_value=float('inf'))\n",
        "    yt = tf.reduce_max(logits - mask, axis=1)\n",
        "    yo = tf.reduce_max(logits, axis=1)\n",
        "\n",
        "    # encourage to classify to a wrong category\n",
        "    loss0 = tf.nn.relu(yo - yt + min_prob)\n",
        "\n",
        "    axis = list(range(1, len(xshape)))\n",
        "    ord_ = float(ord_)\n",
        "\n",
        "    # make sure the adversarial images are visually close\n",
        "    if 2 == ord_:\n",
        "        # CW-L2 Original paper uses the reduce_sum version.  These two\n",
        "        # implementation does not differ much.\n",
        "\n",
        "        # loss1 = tf.reduce_sum(tf.square(xadv-x), axis=axis)\n",
        "        loss1 = tf.reduce_mean(tf.square(xadv-x))\n",
        "    else:\n",
        "        # CW-Linf\n",
        "        tau0 = tf.fill([xshape[0]] + [1]*len(axis), clip[1])\n",
        "        tau = tf.get_variable('cw8-noise-upperbound', dtype=tf.float32,\n",
        "                              initializer=tau0, trainable=False)\n",
        "        diff = xadv - x - tau\n",
        "\n",
        "        # if all values are smaller than the upper bound value tau, we reduce\n",
        "        # this value via tau*0.9 to make sure L-inf does not get stuck.\n",
        "        tau = alpha * tf.to_float(tf.reduce_all(diff < 0, axis=axis))\n",
        "        loss1 = tf.nn.relu(tf.reduce_sum(diff, axis=axis))\n",
        "\n",
        "    loss = eps*loss0 + loss1\n",
        "    train_op = optimizer.minimize(loss, var_list=[noise])\n",
        "\n",
        "    # We may need to update tau after each iteration.  Refer to the CW-Linf\n",
        "    # section in the original paper.\n",
        "    if 2 != ord_:\n",
        "        train_op = tf.group(train_op, tau)\n",
        "    return train_op, xadv, noise"
      ],
      "metadata": {
        "id": "43IHZHMnaItn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "\n",
        "img_size = 28\n",
        "img_chan = 1\n",
        "n_classes = 10\n",
        "batch_size = 64"
      ],
      "metadata": {
        "id": "Ta8qBSuNakLb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Timer(object):\n",
        "    def __init__(self, msg='Starting.....', timer=default_timer, factor=1,\n",
        "                 fmt=\"------- elapsed {:.4f}s --------\"):\n",
        "        self.timer = timer\n",
        "        self.factor = factor\n",
        "        self.fmt = fmt\n",
        "        self.end = None\n",
        "        self.msg = msg\n",
        "\n",
        "    def __call__(self):\n",
        "        \"\"\"\n",
        "        Return the current time\n",
        "        \"\"\"\n",
        "        return self.timer()\n",
        "\n",
        "    def __enter__(self):\n",
        "        \"\"\"\n",
        "        Set the start time\n",
        "        \"\"\"\n",
        "        print(self.msg)\n",
        "        self.start = self()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
        "        \"\"\"\n",
        "        Set the end time\n",
        "        \"\"\"\n",
        "        self.end = self()\n",
        "        print(str(self))\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.fmt.format(self.elapsed)\n",
        "\n",
        "    @property\n",
        "    def elapsed(self):\n",
        "        if self.end is None:\n",
        "            # if elapsed is called in the context manager scope\n",
        "            return (self() - self.start) * self.factor\n",
        "        else:\n",
        "            # if elapsed is called out of the context manager scope\n",
        "            return (self.end - self.start) * self.factor"
      ],
      "metadata": {
        "id": "f2y2wDuhaotF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nLoading MNIST')\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = np.reshape(X_train, [-1, img_size, img_size, img_chan])\n",
        "X_train = X_train.astype(np.float32) / 255\n",
        "X_test = np.reshape(X_test, [-1, img_size, img_size, img_chan])\n",
        "X_test = X_test.astype(np.float32) / 255\n",
        "\n",
        "to_categorical = tf.keras.utils.to_categorical\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giIEKqLwasHi",
        "outputId": "13e40c55-3a3f-4cfe-a5bb-173bcb1e9b58"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading MNIST\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nSpliting data')\n",
        "\n",
        "ind = np.random.permutation(X_train.shape[0])\n",
        "X_train, y_train = X_train[ind], y_train[ind]\n",
        "\n",
        "VALIDATION_SPLIT = 0.1\n",
        "n = int(X_train.shape[0] * (1-VALIDATION_SPLIT))\n",
        "X_valid = X_train[n:]\n",
        "X_train = X_train[:n]\n",
        "y_valid = y_train[n:]\n",
        "y_train = y_train[:n]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KysCdMzFavUu",
        "outputId": "4d1ff539-d1cc-48e8-b8ce-d4ff3b7f2d94"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Spliting data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nConstruction graph')\n",
        "\n",
        "\n",
        "def model(x, logits=False, training=False):\n",
        "    with tf.variable_scope('conv0'):\n",
        "        z = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3],\n",
        "                             padding='same', activation=tf.nn.relu)\n",
        "        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n",
        "\n",
        "    with tf.variable_scope('conv1'):\n",
        "        z = tf.layers.conv2d(z, filters=64, kernel_size=[3, 3],\n",
        "                             padding='same', activation=tf.nn.relu)\n",
        "        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n",
        "\n",
        "    with tf.variable_scope('flatten'):\n",
        "        shape = z.get_shape().as_list()\n",
        "        z = tf.reshape(z, [-1, np.prod(shape[1:])])\n",
        "\n",
        "    with tf.variable_scope('mlp'):\n",
        "        z = tf.layers.dense(z, units=128, activation=tf.nn.relu)\n",
        "        z = tf.layers.dropout(z, rate=0.25, training=training)\n",
        "\n",
        "    logits_ = tf.layers.dense(z, units=10, name='logits')\n",
        "    y = tf.nn.softmax(logits_, name='ybar')\n",
        "\n",
        "    if logits:\n",
        "        return y, logits_\n",
        "    return y\n",
        "\n",
        "\n",
        "class Dummy:\n",
        "    pass\n",
        "\n",
        "\n",
        "env = Dummy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMcEFLmca08r",
        "outputId": "e684a709-fa8f-4fee-d40e-a4afd4392066"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Construction graph\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.variable_scope('model', reuse=tf.AUTO_REUSE):\n",
        "    env.x = tf.placeholder(tf.float32, (None, img_size, img_size, img_chan),\n",
        "                           name='x')\n",
        "    env.y = tf.placeholder(tf.float32, (None, n_classes), name='y')\n",
        "    env.training = tf.placeholder_with_default(False, (), name='mode')\n",
        "\n",
        "    env.ybar, logits = model(env.x, logits=True, training=env.training)\n",
        "\n",
        "    with tf.variable_scope('acc'):\n",
        "        count = tf.equal(tf.argmax(env.y, axis=1), tf.argmax(env.ybar, axis=1))\n",
        "        env.acc = tf.reduce_mean(tf.cast(count, tf.float32), name='acc')\n",
        "\n",
        "    with tf.variable_scope('loss'):\n",
        "        xent = tf.nn.softmax_cross_entropy_with_logits(labels=env.y,\n",
        "                                                       logits=logits)\n",
        "        env.loss = tf.reduce_mean(xent, name='loss')\n",
        "\n",
        "    with tf.variable_scope('train_op'):\n",
        "        optimizer = tf.train.AdamOptimizer()\n",
        "        vs = tf.global_variables()\n",
        "        env.train_op = optimizer.minimize(env.loss, var_list=vs)\n",
        "\n",
        "    env.saver = tf.train.Saver()\n",
        "\n",
        "    # Note here that the shape has to be fixed during the graph construction\n",
        "    # since the internal variable depends upon the shape.\n",
        "    env.x_fixed = tf.placeholder(\n",
        "        tf.float32, (batch_size, img_size, img_size, img_chan),\n",
        "        name='x_fixed')\n",
        "    env.adv_eps = tf.placeholder(tf.float32, (), name='adv_eps')\n",
        "    env.adv_y = tf.placeholder(tf.int32, (), name='adv_y')\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=0.1)\n",
        "    env.adv_train_op, env.xadv, env.noise = cw(model, env.x_fixed, ord_='inf',\n",
        "                                               y=env.adv_y, eps=env.adv_eps,\n",
        "                                               optimizer=optimizer)\n",
        "\n",
        "print('\\nInitializing graph')\n",
        "\n",
        "env.sess = tf.InteractiveSession()\n",
        "env.sess.run(tf.global_variables_initializer())\n",
        "env.sess.run(tf.local_variables_initializer())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1dyALfOa5yE",
        "outputId": "96fe56ff-444d-4281-8204-faacec66a51c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-37014912c8aa>:6: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  z = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3],\n",
            "<ipython-input-8-37014912c8aa>:8: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "  z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n",
            "<ipython-input-8-37014912c8aa>:11: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  z = tf.layers.conv2d(z, filters=64, kernel_size=[3, 3],\n",
            "<ipython-input-8-37014912c8aa>:13: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
            "  z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n",
            "<ipython-input-8-37014912c8aa>:20: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  z = tf.layers.dense(z, units=128, activation=tf.nn.relu)\n",
            "<ipython-input-8-37014912c8aa>:21: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  z = tf.layers.dropout(z, rate=0.25, training=training)\n",
            "<ipython-input-8-37014912c8aa>:23: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  logits_ = tf.layers.dense(z, units=10, name='logits')\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/util/dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initializing graph\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(env, X_data, y_data, batch_size=128):\n",
        "    \"\"\"\n",
        "    Evaluate TF model by running env.loss and env.acc.\n",
        "    \"\"\"\n",
        "    print('\\nEvaluating')\n",
        "\n",
        "    n_sample = X_data.shape[0]\n",
        "    n_batch = int((n_sample+batch_size-1) / batch_size)\n",
        "    loss, acc = 0, 0\n",
        "\n",
        "    for batch in range(n_batch):\n",
        "        print(' batch {0}/{1}'.format(batch + 1, n_batch), end='\\r')\n",
        "        start = batch * batch_size\n",
        "        end = min(n_sample, start + batch_size)\n",
        "        cnt = end - start\n",
        "        batch_loss, batch_acc = env.sess.run(\n",
        "            [env.loss, env.acc],\n",
        "            feed_dict={env.x: X_data[start:end],\n",
        "                       env.y: y_data[start:end]})\n",
        "        loss += batch_loss * cnt\n",
        "        acc += batch_acc * cnt\n",
        "    loss /= n_sample\n",
        "    acc /= n_sample\n",
        "\n",
        "    print(' loss: {0:.4f} acc: {1:.4f}'.format(loss, acc))\n",
        "    return loss, acc"
      ],
      "metadata": {
        "id": "dL3QTtX4a-yj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(env, X_data, y_data, X_valid=None, y_valid=None, epochs=1,\n",
        "          load=False, shuffle=True, batch_size=128, name='model'):\n",
        "    \"\"\"\n",
        "    Train a TF model by running env.train_op.\n",
        "    \"\"\"\n",
        "    if load:\n",
        "        if not hasattr(env, 'saver'):\n",
        "            return print('\\nError: cannot find saver op')\n",
        "        print('\\nLoading saved model')\n",
        "        return env.saver.restore(env.sess, 'model/{}'.format(name))\n",
        "\n",
        "    print('\\nTrain model')\n",
        "    n_sample = X_data.shape[0]\n",
        "    n_batch = int((n_sample+batch_size-1) / batch_size)\n",
        "    for epoch in range(epochs):\n",
        "        print('\\nEpoch {0}/{1}'.format(epoch + 1, epochs))\n",
        "\n",
        "        if shuffle:\n",
        "            print('\\nShuffling data')\n",
        "            ind = np.arange(n_sample)\n",
        "            np.random.shuffle(ind)\n",
        "            X_data = X_data[ind]\n",
        "            y_data = y_data[ind]\n",
        "\n",
        "        for batch in range(n_batch):\n",
        "            print(' batch {0}/{1}'.format(batch + 1, n_batch), end='\\r')\n",
        "            start = batch * batch_size\n",
        "            end = min(n_sample, start + batch_size)\n",
        "            env.sess.run(env.train_op, feed_dict={env.x: X_data[start:end],\n",
        "                                                  env.y: y_data[start:end],\n",
        "                                                  env.training: True})\n",
        "        if X_valid is not None:\n",
        "            evaluate(env, X_valid, y_valid)\n",
        "\n",
        "    if hasattr(env, 'saver'):\n",
        "        print('\\n Saving model')\n",
        "        os.makedirs('model', exist_ok=True)\n",
        "        env.saver.save(env.sess, 'model/{}'.format(name))"
      ],
      "metadata": {
        "id": "_Y0K27L-bCaa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(env, X_data, batch_size=128):\n",
        "    \"\"\"\n",
        "    Do inference by running env.ybar.\n",
        "    \"\"\"\n",
        "    print('\\nPredicting')\n",
        "    n_classes = env.ybar.get_shape().as_list()[1]\n",
        "\n",
        "    n_sample = X_data.shape[0]\n",
        "    n_batch = int((n_sample+batch_size-1) / batch_size)\n",
        "    yval = np.empty((n_sample, n_classes))\n",
        "\n",
        "    for batch in range(n_batch):\n",
        "        print(' batch {0}/{1}'.format(batch + 1, n_batch), end='\\r')\n",
        "        start = batch * batch_size\n",
        "        end = min(n_sample, start + batch_size)\n",
        "        y_batch = env.sess.run(env.ybar, feed_dict={env.x: X_data[start:end]})\n",
        "        yval[start:end] = y_batch\n",
        "    print()\n",
        "    return yval"
      ],
      "metadata": {
        "id": "_Oabk_lZbFqv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_cw(env, X_data, epochs=1, eps=0.1, batch_size=batch_size):\n",
        "    \"\"\"\n",
        "    Generate adversarial via CW optimization.\n",
        "    \"\"\"\n",
        "    print('\\nMaking adversarials via CW')\n",
        "\n",
        "    n_sample = X_data.shape[0]\n",
        "    n_batch = int((n_sample + batch_size - 1) / batch_size)\n",
        "    X_adv = np.empty_like(X_data)\n",
        "\n",
        "\n",
        "    # for batch in range(n_batch):\n",
        "    for batch in range(n_batch):\n",
        "        with Timer('Batch {0}/{1}   '.format(batch + 1, n_batch)):\n",
        "            end = min(n_sample, (batch+1) * batch_size)\n",
        "            start = end - batch_size\n",
        "            feed_dict = {\n",
        "                env.x_fixed: X_data[start:end],\n",
        "                env.adv_eps: eps,\n",
        "                # env.adv_y: np.random.choice(n_classes)\n",
        "                env.adv_y:5\n",
        "                }\n",
        "\n",
        "            env.sess.run(env.noise.initializer)\n",
        "            for epoch in range(epochs):\n",
        "                env.sess.run(env.adv_train_op, feed_dict=feed_dict)\n",
        "\n",
        "            xadv = env.sess.run(env.xadv, feed_dict=feed_dict)\n",
        "            X_adv[start:end] = xadv\n",
        "\n",
        "    return X_adv"
      ],
      "metadata": {
        "id": "7hnLl8UcbLpz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nTraining')\n",
        "\n",
        "train(env, X_train, y_train, X_valid, y_valid, load=False, epochs=5,name='mnist')\n",
        "\n",
        "print('\\nEvaluating on clean data')\n",
        "\n",
        "evaluate(env, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0UD360MbO7O",
        "outputId": "ae9506a2-83ba-4bd6-f5d8-4b4244f74280"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training\n",
            "\n",
            "Train model\n",
            "\n",
            "Epoch 1/5\n",
            "\n",
            "Shuffling data\n",
            "\n",
            "Evaluating\n",
            " loss: 0.0662 acc: 0.9785\n",
            "\n",
            "Epoch 2/5\n",
            "\n",
            "Shuffling data\n",
            " batch 422/422\n",
            "Evaluating\n",
            " loss: 0.0471 acc: 0.9872\n",
            "\n",
            "Epoch 3/5\n",
            "\n",
            "Shuffling data\n",
            "\n",
            "Evaluating\n",
            " loss: 0.0419 acc: 0.9883\n",
            "\n",
            "Epoch 4/5\n",
            "\n",
            "Shuffling data\n",
            "\n",
            "Evaluating\n",
            " loss: 0.0395 acc: 0.9875\n",
            "\n",
            "Epoch 5/5\n",
            "\n",
            "Shuffling data\n",
            "\n",
            "Evaluating\n",
            " loss: 0.0388 acc: 0.9885\n",
            "\n",
            " Saving model\n",
            "\n",
            "Evaluating on clean data\n",
            "\n",
            "Evaluating\n",
            " loss: 0.0290 acc: 0.9902\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.02896733849699376, 0.9902)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nGenerating adversarial data')\n",
        "\n",
        "# It takes a while to run through the full dataset, thus, we demo the result\n",
        "# through a smaller dataset.  We could actually find the best parameter\n",
        "# configuration on a smaller dataset, and then apply to the full dataset.\n",
        "n_sample = 128\n",
        "ind = np.random.choice(X_test.shape[0], size=n_sample)\n",
        "X_test = X_test[ind]\n",
        "y_test = y_test[ind]\n",
        "\n",
        "X_adv = make_cw(env, X_test, eps=1, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRvRUu_NbTYe",
        "outputId": "c13cf46a-01ed-4d9a-c5e6-c9bf7531449a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating adversarial data\n",
            "\n",
            "Making adversarials via CW\n",
            "Batch 1/2   \n",
            "------- elapsed 10.2603s --------\n",
            "Batch 2/2   \n",
            "------- elapsed 12.1275s --------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nEvaluating on adversarial data')\n",
        "\n",
        "evaluate(env, X_adv, y_test)\n",
        "\n",
        "print('\\nRandomly sample adversarial data from each category')\n",
        "\n",
        "y1 = predict(env, X_test)\n",
        "y2 = predict(env, X_adv)\n",
        "\n",
        "z0 = np.argmax(y_test, axis=1)\n",
        "z1 = np.argmax(y1, axis=1)\n",
        "z2 = np.argmax(y2, axis=1)\n",
        "\n",
        "ind = np.logical_and(z0 == z1, z1 != z2)\n",
        "# print('success: ', np.sum(ind))\n",
        "\n",
        "ind = z0 == z1\n",
        "\n",
        "#To save in pickle file\n",
        "X_adv_org = X_adv\n",
        "label_org = z1\n",
        "\n",
        "X_test = X_test[ind]\n",
        "X_adv = X_adv[ind]\n",
        "z1 = z1[ind]\n",
        "z2 = z2[ind]\n",
        "y2 = y2[ind]\n",
        "\n",
        "ind, = np.where(z1 != z2)\n",
        "cur = np.random.choice(ind, size=n_classes)\n",
        "X_org = np.squeeze(X_test[cur])\n",
        "X_tmp = np.squeeze(X_adv[cur])\n",
        "y_tmp = y2[cur]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AF0GnGgbV8M",
        "outputId": "3c1f74b6-f7cd-41fa-c4f0-ab836a2b1d77"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on adversarial data\n",
            "\n",
            "Evaluating\n",
            " batch 1/1\r loss: 7.2828 acc: 0.0859\n",
            "\n",
            "Randomly sample adversarial data from each category\n",
            "\n",
            "Predicting\n",
            " batch 1/1\r\n",
            "\n",
            "Predicting\n",
            " batch 1/1\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_adv_org))\n",
        "print(len(label_org))\n",
        "print(len(X_adv))\n",
        "print(len(z0))\n",
        "print(len(z1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmZTGu8JbbDA",
        "outputId": "82b682b8-067f-4400-a656-c1fd5e0b6a7a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "128\n",
            "128\n",
            "127\n",
            "128\n",
            "127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(10, 2.2))\n",
        "gs = gridspec.GridSpec(2, 10, wspace=0.05, hspace=0.05)\n",
        "\n",
        "label = np.argmax(y_tmp, axis=1)\n",
        "proba = np.max(y_tmp, axis=1)\n",
        "for i in range(10):\n",
        "  ax = fig.add_subplot(gs[0, i])\n",
        "  ax.imshow(X_org[i], cmap='gray', interpolation='none')\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "\n",
        "  ax = fig.add_subplot(gs[1, i])\n",
        "  ax.imshow(X_tmp[i], cmap='gray', interpolation='none')\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "\n",
        "  ax.set_xlabel('{0} ({1:.2f})'.format(label[i], proba[i]), fontsize=12)\n",
        "\n",
        "print('\\nSaving figure')\n",
        "\n",
        "gs.tight_layout(fig)\n",
        "os.makedirs('img/', exist_ok=True)\n",
        "plt.savefig('img/cw8_mnist.png')\n",
        "# from IPython.display import Image\n",
        "# Image('img/cw8_mnist.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elcR1lRRbgEm",
        "outputId": "0ebecc06-5f09-4840-9951-1d3caf1a5556"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saving figure\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "db = {}\n",
        "db['x_adv'] = X_adv\n",
        "db['label'] = z1\n",
        "db['x_org'] = X_org\n",
        "db['prob'] = proba\n",
        "\n",
        "# Its important to use binary mode\n",
        "dbfile = open('images.pkl', 'ab')\n",
        "      \n",
        "# source, destination\n",
        "pickle.dump(db, dbfile)                     \n",
        "dbfile.close()"
      ],
      "metadata": {
        "id": "HBQr15Xibjon"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}